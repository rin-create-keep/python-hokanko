import os
import json
import streamlit as st
from openai import OpenAI
import anthropic
import google.generativeai as genai
from datetime import datetime
import time
from io import BytesIO
import base64

# dotenvã‚’åˆ©ç”¨ã—ãªã„å ´åˆã¯æ¶ˆã—ã¦ãã ã•ã„
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    import warnings
    warnings.warn("dotenv not found. Please make sure to set your environment variables manually.", ImportWarning)

# tiktokenã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ(ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ­£ç¢ºè¨ˆç®—ç”¨)
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    import warnings
    warnings.warn("tiktoken not installed. Using approximate token counting.", ImportWarning)

MODEL_PRICES = {
    "input": {
        "gpt-3.5-turbo": 0.5 / 1_000_000,
        "gpt-4o": 5 / 1_000_000,
        "claude-3-5-sonnet-20241022": 3 / 1_000_000,
        "gemini-1.5-pro-latest": 3.5 / 1_000_000
    },
    "output": {
        "gpt-3.5-turbo": 1.5 / 1_000_000,
        "gpt-4o": 15 / 1_000_000,
        "claude-3-5-sonnet-20241022": 15 / 1_000_000,
        "gemini-1.5-pro-latest": 10.5 / 1_000_000
    }
}

SYSTEM_PROMPTS = {
    "è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ": "You are a helpful assistant.",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å°‚é–€å®¶": "You are an expert programmer who provides clear, concise code solutions with explanations.",
    "ç¿»è¨³è€…": "You are a professional translator. Translate text accurately while preserving the original meaning and tone.",
    "æ–‡ç« æ ¡æ­£è€…": "You are a professional editor. Review and improve text for clarity, grammar, and style.",
    "å‰µé€ çš„ãªä½œå®¶": "You are a creative writer who crafts engaging stories and content.",
    "ã‚«ã‚¹ã‚¿ãƒ ": ""
}

PRESET_PROMPTS = [
    "ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦æ”¹å–„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„",
    "ã“ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„",
    "ã“ã®è‹±æ–‡ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„",
    "ã“ã®ã‚¨ãƒ©ãƒ¼ã®è§£æ±ºæ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
    "ã€œã«ã¤ã„ã¦ç°¡å˜ã«èª¬æ˜ã—ã¦ãã ã•ã„"
]

def get_accurate_token_count(text: str, model: str) -> int:
    """æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    if not text:
        return 0
    
    if not TIKTOKEN_AVAILABLE:
        return max(1, len(text) // 4)
    
    try:
        if model.startswith("gpt"):
            encoding = tiktoken.encoding_for_model(model)
        elif model.startswith("claude"):
            encoding = tiktoken.get_encoding("cl100k_base")
        else:
            encoding = tiktoken.get_encoding("cl100k_base")
        
        return len(encoding.encode(text))
    except Exception:
        return max(1, len(text) // 4)

def save_conversation(conversation_name: str, messages: list, model: str):
    """ä¼šè©±ã‚’ä¿å­˜"""
    if not os.path.exists("conversations"):
        os.makedirs("conversations")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"conversations/{conversation_name}_{timestamp}.json"
    
    data = {
        "name": conversation_name,
        "timestamp": timestamp,
        "model": model,
        "messages": messages
    }
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filename

def load_conversation(filename: str):
    """ä¼šè©±ã‚’èª­ã¿è¾¼ã¿"""
    with open(filename, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

def list_saved_conversations():
    """ä¿å­˜ã•ã‚ŒãŸä¼šè©±ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
    if not os.path.exists("conversations"):
        return []
    
    files = [f for f in os.listdir("conversations") if f.endswith(".json")]
    conversations = []
    
    for f in files:
        try:
            with open(f"conversations/{f}", "r", encoding="utf-8") as file:
                data = json.load(file)
                conversations.append({
                    "filename": f,
                    "name": data.get("name", "Unnamed"),
                    "timestamp": data.get("timestamp", ""),
                    "model": data.get("model", "")
                })
        except Exception:
            continue
    
    return sorted(conversations, key=lambda x: x["timestamp"], reverse=True)

def export_conversation_as_markdown(messages: list) -> str:
    """ä¼šè©±ã‚’Markdownå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    md = f"# Chat Export - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    for role, message in messages:
        if role == "system":
            md += f"**System Prompt:** {message}\n\n---\n\n"
        elif role == "user":
            md += f"## ğŸ‘¤ User\n\n{message}\n\n"
        elif role == "assistant":
            md += f"## ğŸ¤– Assistant\n\n{message}\n\n"
    
    return md

def process_uploaded_file(uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    file_type = uploaded_file.type
    
    if file_type.startswith("text"):
        content = uploaded_file.read().decode("utf-8")
        return f"[ãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name}]\n\n{content}"
    elif file_type == "application/pdf":
        return f"[PDFãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name}] â€»PDFå†…å®¹ã®èª­ã¿å–ã‚Šã«ã¯è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™"
    elif file_type.startswith("image"):
        bytes_data = uploaded_file.read()
        base64_image = base64.b64encode(bytes_data).decode()
        return f"[ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name}]", base64_image
    else:
        return f"[ãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name}] â€»ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã¯æœªå¯¾å¿œã§ã™"

def init_page():
    st.set_page_config(
        page_title="My Great ChatGPT Pro",
        page_icon="ğŸ¤—",
        layout="wide"
    )
    st.header("My Great ChatGPT Pro ğŸ¤—")

def init_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–"""
    if "conversations" not in st.session_state:
        st.session_state.conversations = {
            "conversation_1": {
                "name": "ä¼šè©± 1",
                "messages": [("system", "You are a helpful assistant.")],
                "created_at": datetime.now().isoformat()
            }
        }
    
    if "current_conversation" not in st.session_state:
        st.session_state.current_conversation = "conversation_1"
    
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.0
    
    if "model_name" not in st.session_state:
        st.session_state.model_name = "gpt-3.5-turbo"
    
    if "system_prompt_type" not in st.session_state:
        st.session_state.system_prompt_type = "è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
    
    if "custom_system_prompt" not in st.session_state:
        st.session_state.custom_system_prompt = ""
    
    if "usage_stats" not in st.session_state:
        st.session_state.usage_stats = {
            "daily": {},
            "model_usage": {}
        }

def get_current_messages():
    """ç¾åœ¨ã®ä¼šè©±ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—"""
    return st.session_state.conversations[st.session_state.current_conversation]["messages"]

def set_current_messages(messages):
    """ç¾åœ¨ã®ä¼šè©±ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š"""
    st.session_state.conversations[st.session_state.current_conversation]["messages"] = messages

def sidebar_controls():
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«"""
    st.sidebar.title("ğŸ›ï¸ Options")
    
    # ä¼šè©±ç®¡ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.sidebar.markdown("### ğŸ’¬ ä¼šè©±ç®¡ç†")
    
    # ç¾åœ¨ã®ä¼šè©±é¸æŠ
    conversation_names = {
        conv_id: conv["name"] 
        for conv_id, conv in st.session_state.conversations.items()
    }
    
    selected = st.sidebar.selectbox(
        "ä¼šè©±ã‚’é¸æŠ",
        options=list(conversation_names.keys()),
        format_func=lambda x: conversation_names[x],
        key="conversation_selector"
    )
    
    if selected != st.session_state.current_conversation:
        st.session_state.current_conversation = selected
        st.rerun()
    
    col1, col2 = st.sidebar.columns(2)
    
    with col1:
        if st.button("â• æ–°è¦ä¼šè©±", use_container_width=True):
            new_id = f"conversation_{len(st.session_state.conversations) + 1}"
            system_prompt = SYSTEM_PROMPTS[st.session_state.system_prompt_type]
            if st.session_state.system_prompt_type == "ã‚«ã‚¹ã‚¿ãƒ ":
                system_prompt = st.session_state.custom_system_prompt
            
            st.session_state.conversations[new_id] = {
                "name": f"ä¼šè©± {len(st.session_state.conversations) + 1}",
                "messages": [("system", system_prompt)],
                "created_at": datetime.now().isoformat()
            }
            st.session_state.current_conversation = new_id
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ å‰Šé™¤", use_container_width=True):
            if len(st.session_state.conversations) > 1:
                del st.session_state.conversations[st.session_state.current_conversation]
                st.session_state.current_conversation = list(st.session_state.conversations.keys())[0]
                st.rerun()
    
    # ä¼šè©±åã®å¤‰æ›´
    new_name = st.sidebar.text_input(
        "ä¼šè©±åã‚’å¤‰æ›´",
        value=st.session_state.conversations[st.session_state.current_conversation]["name"]
    )
    if new_name != st.session_state.conversations[st.session_state.current_conversation]["name"]:
        st.session_state.conversations[st.session_state.current_conversation]["name"] = new_name
    
    st.sidebar.markdown("---")
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    st.sidebar.markdown("### ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    
    model = st.sidebar.radio(
        "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        ("GPT-3.5", "GPT-4", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    )
    
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo"
    elif model == "GPT-4":
        st.session_state.model_name = "gpt-4o"
    elif model == "Claude 3.5 Sonnet":
        st.session_state.model_name = "claude-3-5-sonnet-20241022"
    else:
        st.session_state.model_name = "gemini-1.5-pro-latest"
    
    st.session_state.temperature = st.sidebar.slider(
        "Temperature", 0.0, 2.0, st.session_state.temperature, 0.01
    )
    
    st.sidebar.markdown("---")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
    st.sidebar.markdown("### ğŸ“ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    
    prompt_type = st.sidebar.selectbox(
        "ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠ",
        options=list(SYSTEM_PROMPTS.keys())
    )
    
    st.session_state.system_prompt_type = prompt_type
    
    if prompt_type == "ã‚«ã‚¹ã‚¿ãƒ ":
        st.session_state.custom_system_prompt = st.sidebar.text_area(
            "ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            value=st.session_state.custom_system_prompt,
            height=100
        )
        current_system_prompt = st.session_state.custom_system_prompt
    else:
        current_system_prompt = SYSTEM_PROMPTS[prompt_type]
    
    if st.sidebar.button("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é©ç”¨"):
        messages = get_current_messages()
        messages[0] = ("system", current_system_prompt)
        set_current_messages(messages)
        st.success("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    
    st.sidebar.markdown("---")
    
    # ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿
    st.sidebar.markdown("### ğŸ’¾ ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿")
    
    save_name = st.sidebar.text_input("ä¿å­˜å", value="my_conversation")
    if st.sidebar.button("ğŸ’¾ ä¼šè©±ã‚’ä¿å­˜", use_container_width=True):
        try:
            filename = save_conversation(
                save_name,
                get_current_messages(),
                st.session_state.model_name
            )
            st.sidebar.success(f"ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        except Exception as e:
            st.sidebar.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    saved_convs = list_saved_conversations()
    if saved_convs:
        selected_file = st.sidebar.selectbox(
            "ä¿å­˜ã•ã‚ŒãŸä¼šè©±",
            options=[c["filename"] for c in saved_convs],
            format_func=lambda x: next(c["name"] for c in saved_convs if c["filename"] == x)
        )
        
        if st.sidebar.button("ğŸ“‚ ä¼šè©±ã‚’èª­ã¿è¾¼ã¿", use_container_width=True):
            try:
                data = load_conversation(f"conversations/{selected_file}")
                new_id = f"conversation_{len(st.session_state.conversations) + 1}"
                st.session_state.conversations[new_id] = {
                    "name": data["name"],
                    "messages": data["messages"],
                    "created_at": datetime.now().isoformat()
                }
                st.session_state.current_conversation = new_id
                st.session_state.model_name = data.get("model", "gpt-3.5-turbo")
                st.rerun()
            except Exception as e:
                st.sidebar.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    st.sidebar.markdown("---")
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    st.sidebar.markdown("### ğŸ“¤ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    
    if st.sidebar.button("ğŸ“„ Markdownã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", use_container_width=True):
        md_content = export_conversation_as_markdown(get_current_messages())
        st.sidebar.download_button(
            label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=md_content,
            file_name=f"chat_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown"
        )
    
    st.sidebar.markdown("---")
    
    # ã‚³ã‚¹ãƒˆè¡¨ç¤º
    calc_and_display_costs()
    
    # çµ±è¨ˆæƒ…å ±
    display_usage_stats()

def calc_and_display_costs():
    """ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º"""
    messages = get_current_messages()
    output_count = 0
    input_count = 0
    
    for role, message in messages:
        token_count = get_accurate_token_count(message, st.session_state.model_name)
        if role == "assistant":
            output_count += token_count
        else:
            input_count += token_count
    
    if len(messages) <= 1:
        return
    
    input_cost = MODEL_PRICES['input'][st.session_state.model_name] * input_count
    output_cost = MODEL_PRICES['output'][st.session_state.model_name] * output_count
    
    if "gemini" in st.session_state.model_name and (input_count + output_count) > 128000:
        input_cost *= 2
        output_cost *= 2
    
    cost = output_cost + input_cost
    
    st.sidebar.markdown("### ğŸ’° ã‚³ã‚¹ãƒˆ")
    st.sidebar.markdown(f"**åˆè¨ˆ: ${cost:.5f}**")
    st.sidebar.markdown(f"- å…¥åŠ›: ${input_cost:.5f} ({input_count} tokens)")
    st.sidebar.markdown(f"- å‡ºåŠ›: ${output_cost:.5f} ({output_count} tokens)")

def display_usage_stats():
    """ä½¿ç”¨çµ±è¨ˆã‚’è¡¨ç¤º"""
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“Š ä½¿ç”¨çµ±è¨ˆ")
    
    # ä»Šæ—¥ã®æ—¥ä»˜
    today = datetime.now().strftime("%Y-%m-%d")
    
    # ãƒ¢ãƒ‡ãƒ«åˆ¥ä½¿ç”¨å›æ•°
    model_stats = st.session_state.usage_stats.get("model_usage", {})
    if model_stats:
        st.sidebar.markdown("**ãƒ¢ãƒ‡ãƒ«åˆ¥ä½¿ç”¨å›æ•°:**")
        for model, count in model_stats.items():
            st.sidebar.markdown(f"- {model}: {count}å›")

def get_llm_response(user_input: str, uploaded_file=None):
    """LLMã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—(ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆ)"""
    model = st.session_state.model_name
    messages = get_current_messages()
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä»¥å¤–ã‚’æŠ½å‡º
    chat_messages = [
        {"role": role, "content": msg}
        for role, msg in messages
        if role != "system"
    ]
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
    system_prompt = next((msg for role, msg in messages if role == "system"), "You are a helpful assistant.")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ·»ä»˜ã•ã‚Œã¦ã„ã‚‹å ´åˆ
    if uploaded_file:
        file_content = process_uploaded_file(uploaded_file)
        if isinstance(file_content, tuple):
            user_input = f"{user_input}\n\n{file_content[0]}"
        else:
            user_input = f"{user_input}\n\n{file_content}"
    
    chat_messages.append({"role": "user", "content": user_input})
    
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            # GPT
            if model.startswith("gpt"):
                api_key = os.environ.get("OPENAI_API_KEY")
                if not api_key:
                    yield "ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                    return
                
                client = OpenAI(api_key=api_key)
                stream = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "system", "content": system_prompt}] + chat_messages,
                    temperature=st.session_state.temperature,
                    stream=True,
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                
                # ä½¿ç”¨çµ±è¨ˆã‚’æ›´æ–°
                update_usage_stats(model)
                return
            
            # Claude
            elif model.startswith("claude"):
                api_key = os.environ.get("ANTHROPIC_API_KEY")
                if not api_key:
                    yield "ã‚¨ãƒ©ãƒ¼: ANTHROPIC_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                    return
                
                client = anthropic.Anthropic(api_key=api_key)
                with client.messages.stream(
                    model=model,
                    max_tokens=4096,
                    system=system_prompt,
                    messages=chat_messages,
                    temperature=st.session_state.temperature,
                ) as stream:
                    for text in stream.text_stream:
                        yield text
                
                # ä½¿ç”¨çµ±è¨ˆã‚’æ›´æ–°
                update_usage_stats(model)
                return
            
            # Gemini
            elif model.startswith("gemini"):
                api_key = os.environ.get("GOOGLE_API_KEY")
                if not api_key:
                    yield "ã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                    return
                
                genai.configure(api_key=api_key)
                gemini_model = genai.GenerativeModel(model)
                response = gemini_model.generate_content(user_input, stream=True)
                
                for chunk in response:
                    if chunk.text:
                        yield chunk.text
                
                # ä½¿ç”¨çµ±è¨ˆã‚’æ›´æ–°
                update_usage_stats(model)
                return
        
        except Exception as e:
            error_msg = str(e)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
            if "rate_limit" in error_msg.lower() or "429" in error_msg:
                if attempt < max_retries - 1:
                    yield f"\n\nâš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚{retry_delay}ç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™...\n\n"
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    yield f"\n\nâŒ ã‚¨ãƒ©ãƒ¼: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
                    return
            
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
            else:
                yield f"\n\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}"
                if attempt < max_retries - 1:
                    yield f"\n\nå†è©¦è¡Œä¸­... ({attempt + 1}/{max_retries})"
                    time.sleep(retry_delay)
                    continue
                else:
                    return

def update_usage_stats(model: str):
    """ä½¿ç”¨çµ±è¨ˆã‚’æ›´æ–°"""
    today = datetime.now().strftime("%Y-%m-%d")
    
    # æ—¥åˆ¥çµ±è¨ˆ
    if today not in st.session_state.usage_stats["daily"]:
        st.session_state.usage_stats["daily"][today] = 0
    st.session_state.usage_stats["daily"][today] += 1
    
    # ãƒ¢ãƒ‡ãƒ«åˆ¥çµ±è¨ˆ
    if model not in st.session_state.usage_stats["model_usage"]:
        st.session_state.usage_stats["model_usage"][model] = 0
    st.session_state.usage_stats["model_usage"][model] += 1

def main():
    init_page()
    init_session_state()
    sidebar_controls()
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    messages = get_current_messages()
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’è¡¨ç¤º
    for role, message in messages:
        if role != "system":
            with st.chat_message(role):
                st.markdown(message)
    
    # ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    with st.expander("ğŸ“Œ ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"):
        cols = st.columns(3)
        for idx, preset in enumerate(PRESET_PROMPTS):
            if cols[idx % 3].button(preset, key=f"preset_{idx}"):
                st.session_state.preset_input = preset
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜ (ãƒ†ã‚­ã‚¹ãƒˆ/ç”»åƒ/PDF)",
        type=["txt", "md", "py", "json", "csv", "pdf", "png", "jpg", "jpeg"]
    )
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    user_input = st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­ï¼")
    
    # ãƒ—ãƒªã‚»ãƒƒãƒˆãŒé¸æŠã•ã‚ŒãŸå ´åˆ
    if "preset_input" in st.session_state:
        user_input = st.session_state.preset_input
        del st.session_state.preset_input
    
    if user_input:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            response_text = ""
            
            for token in get_llm_response(user_input, uploaded_file):
                response_text += token
                response_placeholder.markdown(response_text)
        
        # å±¥æ­´ã«è¿½åŠ 
        messages.append(("user", user_input))
        messages.append(("assistant", response_text))
        set_current_messages(messages)

if __name__ == '__main__':
    main()
