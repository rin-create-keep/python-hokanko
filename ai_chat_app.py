import os
import streamlit as st
from openai import OpenAI
import anthropic
import google.generativeai as genai
import json
import base64
from urllib.parse import urlencode, parse_qs
from io import BytesIO

###### dotenv ã‚’åˆ©ç”¨ã—ãªã„å ´åˆã¯æ¶ˆã—ã¦ãã ã•ã„ ######
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    import warnings
    warnings.warn("dotenv not found. Please make sure to set your environment variables manually.", ImportWarning)
################################################


MODEL_PRICES = {
    "input": {
        "gpt-3.5-turbo": 0.5 / 1_000_000,
        "gpt-4o": 5 / 1_000_000,
        "claude-3-5-sonnet-20241022": 3 / 1_000_000,
        "gemini-1.5-pro-latest": 3.5 / 1_000_000
    },
    "output": {
        "gpt-3.5-turbo": 1.5 / 1_000_000,
        "gpt-4o": 15 / 1_000_000,
        "claude-3-5-sonnet-20241022": 15 / 1_000_000,
        "gemini-1.5-pro-latest": 10.5 / 1_000_000
    }
}

def get_message_counts(text: str) -> int:
    if not text:
        return 0
    return max(1, len(text) // 4)

def init_page():
    st.set_page_config(
        page_title="My Great ChatGPT",
        page_icon="ğŸ¤—"
    )
    st.header("My Great ChatGPT ğŸ¤—")
    st.sidebar.title("Options")


def encode_conversation(message_history):
    """ä¼šè©±å±¥æ­´ã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
    try:
        json_str = json.dumps(message_history, ensure_ascii=False)
        encoded = base64.urlsafe_b64encode(json_str.encode('utf-8')).decode('utf-8')
        return encoded
    except Exception as e:
        st.error(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def decode_conversation(encoded_str):
    """Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸä¼šè©±å±¥æ­´ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰"""
    try:
        json_str = base64.urlsafe_b64decode(encoded_str.encode('utf-8')).decode('utf-8')
        return json.loads(json_str)
    except Exception as e:
        st.error(f"ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def create_share_url():
    """å…±æœ‰ç”¨URLã‚’ç”Ÿæˆ"""
    if "message_history" not in st.session_state:
        return None
    
    encoded = encode_conversation(st.session_state.message_history)
    if encoded:
        # Streamlit Cloudãªã©ã®ãƒ‡ãƒ—ãƒ­ã‚¤å…ˆURLã‚’ä½¿ç”¨
        base_url = st.get_option("browser.serverAddress") or "localhost:8501"
        share_url = f"http://{base_url}?chat={encoded}"
        return share_url
    return None


def load_conversation_from_url():
    """URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ä¼šè©±ã‚’ãƒ­ãƒ¼ãƒ‰"""
    query_params = st.query_params
    if "chat" in query_params:
        encoded = query_params["chat"]
        decoded = decode_conversation(encoded)
        if decoded:
            st.session_state.message_history = decoded
            st.success("ä¼šè©±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")
            # URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
            st.query_params.clear()


def transcribe_audio(audio_file):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—"""
    try:
        client = OpenAI()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        audio_bytes = audio_file.read()
        audio_file.seek(0)  # ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
        
        # Whisper APIã§æ–‡å­—èµ·ã“ã—
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="text"
        )
        
        return transcript
    except Exception as e:
        st.error(f"æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def generate_minutes(transcript):
    """æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è­°äº‹éŒ²ã‚’ç”Ÿæˆ"""
    try:
        client = OpenAI()
        
        prompt = f"""
ä»¥ä¸‹ã¯ä¼šè­°ã®æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã“ã‚Œã‚’èª­ã¿ã‚„ã™ã„è­°äº‹éŒ²å½¢å¼ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã€è¦ä»¶ã€‘
- æ—¥æ™‚ã€å‚åŠ è€…ã€è­°é¡Œã‚’æ¨æ¸¬ã—ã¦è¨˜è¼‰
- ä¸»è¦ãªè­°è«–ãƒã‚¤ãƒ³ãƒˆã‚’ç®‡æ¡æ›¸ã
- æ±ºå®šäº‹é …ã‚’æ˜ç¢ºã«è¨˜è¼‰
- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆèª°ãŒä½•ã‚’ã™ã‚‹ã‹ï¼‰ã‚’æ•´ç†
- æ¬¡å›ã®äºˆå®šãŒã‚ã‚Œã°è¨˜è¼‰

ã€æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã€‘
{transcript}
"""
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªè­°äº‹éŒ²ä½œæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"è­°äº‹éŒ²ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]


def select_model():
    st.session_state.temperature = st.sidebar.slider(
        "Temperature", 0.0, 2.0, 0.0, 0.01
    )

    model = st.sidebar.radio(
        "Choose a model",
        ("GPT-3.5", "GPT-4", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    )
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo"
    elif model == "GPT-4":
        st.session_state.model_name = "gpt-4o"
    elif model == "Claude 3.5 Sonnet":
        st.session_state.model_name = "claude-3-5-sonnet-20241022"
    else:
        st.session_state.model_name = "gemini-1.5-pro-latest"


def get_llm_response(user_input: str) -> str:
    model = st.session_state.model_name
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        *[
            {"role": role, "content": msg}
            for role, msg in st.session_state.message_history
            if role != "system"
        ],
        {"role": "user", "content": user_input}
    ]

    # GPT
    if model.startswith("gpt"):
        client = OpenAI()
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=st.session_state.temperature,
            stream=True,
        )

        response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                response += chunk.choices[0].delta.content
                yield chunk.choices[0].delta.content

    # Claude
    if model.startswith("claude"):
        client = anthropic.Anthropic()
        with client.messages.stream(
            model=model,
            max_tokens=1024,
            messages=messages[1:],
        ) as stream:
            response = ""
            for text in stream.text_stream:
                response += text
                yield text

    # Gemini
    if model.startswith("gemini"):
        genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
        model = genai.GenerativeModel(model)
        response = model.generate_content(user_input, stream=True)
        full = ""
        for chunk in response:
            if chunk.text:
                full += chunk.text
                yield chunk.text


def calc_and_display_costs():
    output_count = 0
    input_count = 0
    for role, message in st.session_state.message_history:
        token_count = get_message_counts(message)
        if role == "assistant":
            output_count += token_count
        else:
            input_count += token_count

    if len(st.session_state.message_history) == 1:
        return

    input_cost = MODEL_PRICES['input'][st.session_state.model_name] * input_count
    output_cost = MODEL_PRICES['output'][st.session_state.model_name] * output_count
    if "gemini" in st.session_state.model_name and (input_count + output_count) > 128000:
        input_cost *= 2
        output_cost *= 2

    cost = output_cost + input_cost

    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${cost:.5f}**")
    st.sidebar.markdown(f"- Input cost: ${input_cost:.5f}")
    st.sidebar.markdown(f"- Output cost: ${output_cost:.5f}")


def main():
    init_page()
    
    # URLã‹ã‚‰ä¼šè©±ã‚’ãƒ­ãƒ¼ãƒ‰
    load_conversation_from_url()
    
    init_messages()
    select_model()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å…±æœ‰æ©Ÿèƒ½ã‚’è¿½åŠ 
    st.sidebar.markdown("---")
    st.sidebar.markdown("## ğŸ”— ä¼šè©±ã®å…±æœ‰")
    if st.sidebar.button("å…±æœ‰URLã‚’ç”Ÿæˆ"):
        share_url = create_share_url()
        if share_url:
            st.sidebar.text_area("å…±æœ‰URL", share_url, height=100)
            st.sidebar.info("ã“ã®URLã‚’ã‚³ãƒ”ãƒ¼ã—ã¦å…±æœ‰ã—ã¦ãã ã•ã„")
    
    # éŸ³å£°è­°äº‹éŒ²æ©Ÿèƒ½
    st.sidebar.markdown("---")
    st.sidebar.markdown("## ğŸ™ï¸ éŸ³å£°è­°äº‹éŒ²")
    audio_file = st.sidebar.file_uploader(
        "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["mp3", "mp4", "mpeg", "mpga", "m4a", "wav", "webm"]
    )
    
    if audio_file and st.sidebar.button("è­°äº‹éŒ²ã‚’ä½œæˆ"):
        with st.spinner("æ–‡å­—èµ·ã“ã—ä¸­..."):
            transcript = transcribe_audio(audio_file)
        
        if transcript:
            st.sidebar.success("æ–‡å­—èµ·ã“ã—å®Œäº†ï¼")
            
            with st.spinner("è­°äº‹éŒ²ã‚’ç”Ÿæˆä¸­..."):
                minutes = generate_minutes(transcript)
            
            if minutes:
                st.sidebar.success("è­°äº‹éŒ²ç”Ÿæˆå®Œäº†ï¼")
                
                # è­°äº‹éŒ²ã‚’è¡¨ç¤º
                st.markdown("## ğŸ“ ç”Ÿæˆã•ã‚ŒãŸè­°äº‹éŒ²")
                st.markdown(minutes)
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                st.download_button(
                    label="è­°äº‹éŒ²ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=minutes,
                    file_name="minutes.txt",
                    mime="text/plain"
                )

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
    for role, message in st.session_state.get("message_history", []):
        if role != "system":
            st.chat_message(role).markdown(message)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    if user_input := st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­ï¼"):
        st.chat_message("user").markdown(user_input)

        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            response_text = ""
            for token in get_llm_response(user_input):
                response_text += token
                response_placeholder.markdown(response_text)

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.message_history.append(("user", user_input))
        st.session_state.message_history.append(("assistant", response_text))

    calc_and_display_costs()

if __name__ == '__main__':
    main()
